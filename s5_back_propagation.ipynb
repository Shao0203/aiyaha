{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ff7646-afc4-4a91-899e-77d70d0648ad",
   "metadata": {},
   "source": [
    "## 第 5 章 误差反向传播法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21e722a-a086-4201-8cc7-8748a5c54bdc",
   "metadata": {},
   "source": [
    "### 5.1 计算图: \n",
    "*计算图将计算过程用图形表示出来, 这里说的图形是数据结构图，通过多个节点和边表示*\n",
    "- 5.1.1 **用计算图求解**: 从左向右进行计算, 为正向传播(forward propagation); 从右向左, 反向传播(backward propagation), 导数计算\n",
    "- 5.1.2 **局部计算**: 与自己相关的某个小范围, 无论全局发生了什么，都能只根据与自己相关的信息输出接下来的结果。\n",
    "- 5.1.3 **为何用计算图解题**: 通过反向传播高效计算导数。(局部计算 + 利用计算图可以将中间的计算结果全部保存起来)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c293ff-0035-413f-86c2-2f2b694e3a29",
   "metadata": {},
   "source": [
    "### 5.2 链式法则: \n",
    "*将局部导数向正方向的反方向（从右到左）传递, 传递这个局部导数的原理，是基于链式法则(chain rule)的。*\n",
    "- 5.2.1 **计算图的反向传播**: 将信号 E 乘以节点的局部导数$(\\frac{∂y}{∂x})$，然后将结果传递给下一个节点。假设 $y=f(x)=x^2$, 则局部导数为$(\\frac{∂y}{∂x})$=2x. 把这个局部导数乘以上游传过来的值(本例中为 E)，然后传递给前面的节点。这就是反向传播的计算顺序。通过这样的计算，可以高效地求出导数的值，这是反向传播的要点。\n",
    "- 5.2.2 **什么是链式法则**: 从复合函数说起, 复合函数是由多个函数构成的函数。比如，$z = (x + y)^2$ 是由 $z = t^2$, $t = x + y$ 两个式子构成的。如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。$(\\frac{∂z}{∂x} = \\frac{∂z}{∂t} · \\frac{∂t}{∂x})$ ∂t 正好可以互相抵消。$(\\frac{∂z}{∂t} = 2t, \\frac{∂t}{∂x} = 1, 2t · 1 = 2(x + y)$。\n",
    "- 5.2.3 **链式法则和计算图**: 计算图的反向传播从右到左传播信号。反向传播的计算顺序是，先将节点的输入信号乘以节点的局部导数（偏导数），然后再传递给下一个节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63220dfe-1335-4b91-8ba5-f78db07bf727",
   "metadata": {},
   "source": [
    "### 5.3 反向传播\n",
    "- 5.3.1 **加法节点的反向传播**: $z = x + y$ 加法节点的反向传播只是将输入信号输出到下一个节点。\n",
    "- 5.3.2 **乘法节点的反向传播**: $z = x · y$ 乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。\n",
    "- 5.3.3 **苹果的例子**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc90aa6d-f908-4911-9c6b-4d2e5686138e",
   "metadata": {},
   "source": [
    "### 5.4 简单层的实现 - 乘法层(MulLayer) & 加法层(AddLayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45490662-cf66-46bd-b081-8bd7c9f89981",
   "metadata": {},
   "source": [
    "#### 5.4.1 乘法层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abbd955-79c3-4926-a7e0-c37685dd43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9adc973-48a3-4c03-8d53-cf3724efde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用2过苹果的例子，买2个苹果，单价100，税率1.1。求最后总价对于这三个元素的导数\n",
    "apple_each = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef8f096-ebea-4d89-8f4c-b3f2c626d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "apple_cost = mul_apple_layer.forward(apple_each, apple_num)\n",
    "total_pay = mul_tax_layer.forward(apple_cost, tax)\n",
    "print(apple_cost, total_pay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcd286e6-1fba-4780-ab07-9df18a129edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "# backward\n",
    "dpay = 1\n",
    "dapple_cost, dtax = mul_tax_layer.backward(dpay)\n",
    "dapple_each, dapple_num = mul_apple_layer.backward(dapple_cost)\n",
    "print(dapple_each, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524aacb-932e-4e4f-81da-5612438c2b40",
   "metadata": {},
   "source": [
    "#### 5.4.2 加法层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f462bc33-b7c0-462b-9b4e-310df6f3ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70897b5-a19a-4e74-a7ac-2c9872b18d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 购买2个苹果(100)和3个橘子(150)\n",
    "apple_each = 100\n",
    "orange_each = 150\n",
    "apple_num = 2\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# Layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1052c81b-9d1e-4fb9-b749-191dffca1e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 450 650 715\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "apple_cost = mul_apple_layer.forward(apple_each, apple_num)\n",
    "orange_cost = mul_orange_layer.forward(orange_each, orange_num)\n",
    "all_cost = add_apple_orange_layer.forward(apple_cost, orange_cost)\n",
    "final_pay = mul_tax_layer.forward(all_cost, tax)\n",
    "print(apple_cost, orange_cost, all_cost, int(final_pay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32cb95eb-f884-48fb-a77b-393de825c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtax: 650\n",
      "dall_cost: 1.1\n",
      "dapple_cost: 1.1\n",
      "dorange_cost: 1.1\n",
      "dorange_each: 3.3\n",
      "dorange_num: 165\n",
      "dapple_each: 2.2\n",
      "dapple_num: 110\n"
     ]
    }
   ],
   "source": [
    "# backward\n",
    "dpay = 1\n",
    "dall_cost, dtax = mul_tax_layer.backward(dpay)\n",
    "dapple_cost, dorange_cost = add_apple_orange_layer.backward(dall_cost)\n",
    "dorange_each, dorange_num = mul_orange_layer.backward(dorange_cost)\n",
    "dapple_each, dapple_num = mul_apple_layer.backward(dapple_cost)\n",
    "\n",
    "print(f'dtax: {dtax}')\n",
    "print(f'dall_cost: {dall_cost}')\n",
    "print(f'dapple_cost: {dapple_cost}')\n",
    "print(f'dorange_cost: {dorange_cost}')\n",
    "print(f'dorange_each: {round(dorange_each, 10)}')\n",
    "print(f'dorange_num: {int(dorange_num)}')\n",
    "print(f'dapple_each: {dapple_each}')\n",
    "print(f'dapple_num: {int(dapple_num)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9f134b-c0ad-4fd9-a9e1-3b46c282a5ee",
   "metadata": {},
   "source": [
    "### 5.5 激活函数层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1613823-19ff-4817-bee2-985b865f09a5",
   "metadata": {},
   "source": [
    "#### 5.5.1 ReLU层\n",
    "$y = \\begin{cases} x, & x > 0 \\\\ 0, & x \\le 0 \\end{cases}$ ， \n",
    "$\\frac{\\partial y}{\\partial x} = \\begin{cases} 1, & x > 0 \\\\ 0, & x \\le 0 \\end{cases}$  \n",
    "如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。  \n",
    "反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将**停在此处**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ac6fc80-7e8d-45d8-a0d1-6182fc0bac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)  # 记录哪些位置≤0\n",
    "        out = x.copy()        # 复制输入\n",
    "        out[self.mask] = 0    # 将≤0的位置设为0\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0   # 将mask标记的位置梯度设为0\n",
    "        dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89ad6da9-074a-46bb-a5f4-15a4e13fcd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 前向传播 ===\n",
      "输入: \n",
      " [[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "mask: \n",
      " [[False  True]\n",
      " [ True False]]\n",
      "输出: \n",
      " [[1. 0.]\n",
      " [0. 3.]]\n",
      "\n",
      "=== 反向传播 ===\n",
      "上游梯度: \n",
      " [[0.5 1. ]\n",
      " [2.  0.8]]\n",
      "本地梯度: \n",
      " [[0.5 0. ]\n",
      " [0.  0.8]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "relu = Relu()\n",
    "\n",
    "# 正向传播时保存反向传播需要的信息，标记了x中小于0的位置在mask中。在前向传播时精心设计，为反向传播做好准备\n",
    "out = relu.forward(x)\n",
    "print(\"=== 前向传播 ===\")\n",
    "print(f\"输入: \\n {x}\")\n",
    "print(f\"mask: \\n {relu.mask}\")\n",
    "print(f\"输出: \\n {out}\")\n",
    "\n",
    "# 正向传播时为0的位置，反向传播时仍为0，其他位置值不变直接传递\n",
    "print(\"\\n=== 反向传播 ===\")\n",
    "dout = np.array([[0.5, 1.0], [2.0, 0.8]])\n",
    "print(f\"上游梯度: \\n {dout}\")\n",
    "dx = relu.backward(dout)\n",
    "print(f\"本地梯度: \\n {dx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d07478-b607-44e0-8ca5-e187581eaffb",
   "metadata": {},
   "source": [
    "#### 5.5.2 Sigmoid层\n",
    "$y = σ(x) = \\frac{1}{1 + exp(-x)}$ , 那么y对x求导，结果为：$\\frac{dy}{dx} = y·(1 - y)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "908da40d-00e8-4b30-a4eb-4b0ce5ccc3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = 1 / (1 + np.exp(-x))\n",
    "        self.out = y    # 将输出保存\n",
    "        return y\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef90667c-3b59-46b6-92cf-9e83da53f2b1",
   "metadata": {},
   "source": [
    "### 5.6 Affine/Softmax 层的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b5d25-03f7-486a-b62f-f980ce24ec5d",
   "metadata": {},
   "source": [
    "#### 5.6.1 Affine层\n",
    "- 神经元的**加权总和** $Y = X \\cdot W + B$, 然后 Y 经过激活函数转换后，传递给下一层。这就是神经网络正向传播的流程。\n",
    "- 神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”,因此，这里将进行仿射变换的处理实现为“Affine 层”。\n",
    "$$ \\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T $$\n",
    "$$ \\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y} $$\n",
    "$$ W = \\begin{pmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{pmatrix}$$\n",
    "$$ W^T = \\begin{pmatrix} w_{11} & w_{21} \\\\ w_{12} & w_{22} \\\\ w_{13} & w_{23} \\end{pmatrix}$$\n",
    "- 如果 $W$ 的形状是 (2, 3)，$W^T$ 的形状就是 (3, 2)\n",
    "- X 和 $\\frac{\\partial L}{\\partial X}$ 形状相同 (2,)\n",
    "- W 和 $\\frac{\\partial L}{\\partial W}$ 形状相同 (2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ba452-897c-479f-a613-d2999fa85092",
   "metadata": {},
   "source": [
    "#### 5.6.2 批版本的Affine层\n",
    "前面介绍的 Affi ne 层的输入 X 是以单个数据为对象的。现在我们考虑 N个数据一起进行正向传播的情况，也就是批版本的 Affine 层。  \n",
    "$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^T$ ------ \n",
    "$\\frac{\\partial L}{\\partial W} = X^T \\cdot \\frac{\\partial L}{\\partial Y}$ ------ \n",
    "$\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y}$  \n",
    "(N,2) = (N,3)·(3,2) ----- (2,3) = (2,N)·(N,3) ----- (3) = (N,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae59292d-8562-4c36-ac8e-c9b95a6c584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n",
      "\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# 偏置B的反向传播值为第一个轴（第 0 轴）方向上的和，即 dB = np.sum(dY, axis=0)\n",
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])\n",
    "print(X_dot_W)\n",
    "print(X_dot_W + B)\n",
    "print()\n",
    "\n",
    "dY = np.array([[1, 2, 3,], [4, 5, 6]])\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dY)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5e1c338-2540-4191-830a-e401a247168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    \"\"\"\n",
    "    Affine层要求二维输入:(batch_size, features)\n",
    "    图像数据通常是多维的: (batch_size, height, width, channels)\n",
    "    reshape(x.shape[0], -1) 的含义：\n",
    "    x.shape[0]：保持批量大小不变\n",
    "    -1：让NumPy自动计算其他维度展平后的长度\n",
    "\n",
    "    比如 x = np.random.rand(2, 3, 4, 5), \n",
    "    print(x.shape)  # (2, 3, 4, 5)\n",
    "    x = x.reshape(x.shape[0], -1)\n",
    "    print(x.shape)  # (2, 60)\n",
    "    x.shape[0]代表输入数据个数，这里是2 代表2条数据；后面的60表示特征features\n",
    "\n",
    "    维度匹配规则:\n",
    "    输入x: (batch_size=2, 3, 4)\n",
    "    输入x: (batch_size, 12)   # reshape 3×4=12\n",
    "    权重W: (12, output_dim=10)   # 必须匹配！\n",
    "    偏置b: (output_dim=10,)\n",
    "    输出: (batch_size, output_dim) # (2, 10)\n",
    "    \"\"\"\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.original_x_shape = None           # 记录原始形状\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape        # 记录原始形状（用于处理非矩阵输入）\n",
    "        self.x = x.reshape(x.shape[0], -1)     # 展平为矩阵\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        dx = dx.reshape(self.original_x_shape) # 恢复原始形状\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e073704e-a520-47ac-a512-de67a84de044",
   "metadata": {},
   "source": [
    "#### 5.6.3 Softmax-with-Loss 层\n",
    "- softmax 函数会将输入值正规化之后再输出。比如手写数字识别时, 输入图像通过Aﬃ ne 层和ReLU 层进行转换，10 个输入通过Softmax 层进行正规化。“0”的得分是 5.3，这个值经过 Softmax 层转换为 0.008（ 0.8% ）；“2”的得分是10.1，被转换为0.991 （ 99.1% ）\n",
    "- Softmax 层将输入值正规化（将输出值的和调整为 1 ）之后再输出。另外，因为手写数字识别要进行 10 类分类，所以向 Softmax 层的输入也有 10 个。\n",
    "- 考虑到这里也包含作为损失函数的交叉熵误差（ cross entropy error ），所以称为“Softmax-with-Loss 层”\n",
    "- Softmax 层的反向传播得到了（ y1 − t1, y2 − t2, y3 − t3 ）这样“漂亮”的结果。由于（ y1, y2, y3 ）是 Softmax 层的输出，（ t1, t2, t3 ）是监督数据，所以（ y1 − t1, y2 − t2, y3 − t3 ）是Softmax层的输出和监督标签的**差分**。神经网络的反向传播会把这个**差分**表示的**误差**传递给前面的层，这是神经网络学习中的重要性质。\n",
    "- 神经网络学习的目的就是通过调整**权重参数**，使神经网络的输出（Softmax的输出）接近监督标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "040e5563-1efe-43fd-add4-7cf8f3483ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None   # 损失\n",
    "        self.y = None      # softmax的输出\n",
    "        self.t = None      # 监督数据（ one-hot vector）\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c50501-397f-4ae5-82cf-35d8de589f6f",
   "metadata": {},
   "source": [
    "### 5.7 误差反向传播法的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a296b-4959-4f8d-b6d2-908683a8e47a",
   "metadata": {},
   "source": [
    "#### 5.7.1 神经网络学习的全貌图\n",
    "- 前提:神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面4个步骤。误差反向传播法会在步骤2中出现。和需要花费较多时间的数值微分不同，误差反向传播法可以快速高效地计算梯度。\n",
    "1. mini-batch: 从训练数据中随机选择一部分数据。\n",
    "2. 计算梯度: 计算损失函数关于各个权重参数的梯度。\n",
    "3. 更新参数: 将权重参数沿梯度方向进行微小的更新。\n",
    "4. 重复: 重复步骤 1、步骤 2、步骤 3。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b219b9c-9b64-459b-b7cc-45f1524a2244",
   "metadata": {},
   "source": [
    "#### 5.7.2 对应误差反向传播法的神经网络的实现\n",
    "- 每个层都精心保存了反向传播需要的信息：\n",
    "    - Affine层: 保存输入x (用于计算dW = x.T · dout)\n",
    "    - ReLU层: 保存mask (用于梯度阻断)  \n",
    "    - Sigmoid层: 保存输出out (用于梯度计算)\n",
    "    - Softmax层: 保存y和t (用于梯度简化)\n",
    "\n",
    "- 每个层的反向传播都对应简洁的数学公式：\n",
    "    - Affine: dx = dout · Wᵀ, dW = xᵀ · dout, db = sum(dout)\n",
    "    - ReLU: dx = dout * (x > 0)\n",
    "    - Sigmoid: dx = dout * (1 - out) * out\n",
    "    - SoftmaxWithLoss: dx = (y - t) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "561ba27d-5cfa-4437-ba5f-266b627f2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from common.layers import Relu, Affine, SoftmaxWithLoss\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss() # 此设计允许灵活替换损失函数，如回归任务 self.lastLayer = MSELoss()\n",
    "    \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x # 第三轮: Affine2.forward(x), np.dot(x, W2) + b2 → 形状: (batch_size, output_size)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)  # y = Affine2的输出（得分），未计算softmax概率\n",
    "        return self.lastLayer.forward(y, t)  # 最后一层先softmax计算预测的的概率y，再与真值t通过交叉熵算出loss\n",
    "        \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)       # 加权输入总和\n",
    "        y = np.argmax(y, axis=1)  # 每个数据中最大值对应位置索引\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1) # 如果t是one-hot-vector，转成整数标签形式\n",
    "        return np.sum(y == t) / x.shape[0]  # 比较预测值与真实标签一致性在全体数据中的占比\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)  # 一个闭包：它记住了当前的x和t值，此处定义函数但不执行，传递给numerical_gradient里面再执行\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"\n",
    "        只需2次传播: 直接计算出所有39,760个梯度\n",
    "        1次前向传播: 输入x → Affine1 → ReLU1 → Affine2 → Softmax → 损失值\n",
    "        1次反向传播: dout=1 → SoftmaxWithLoss → Affine2 → ReLU1 → Affine1\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t) \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        # store gradient\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba8f42-4831-46de-aff5-df6c6ddda0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
